{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ5Rg27yGdWd"
   },
   "source": [
    "# COVID-19 Literature Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjcVT_ieGdWk"
   },
   "source": [
    "# Table of Contents\n",
    "1. Loading the data\n",
    "2. Pre-processing\n",
    "3. Vectorization\n",
    "4. PCA  & Clustering\n",
    "5. Dimensionality Reduction with t-SNE\n",
    "6. Topic Modeling on Each Cluster\n",
    "7. Classify\n",
    "8. Plot\n",
    "9. How to Use the Plot?\n",
    "10. Conclusion\n",
    "11. Citation/Sources\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M_HY5CFGdWl",
    "outputId": "25f9202e-e213-49dd-f80a-46dd4d71dbc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Bio\n",
      "  Downloading bio-0.4.1-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from Bio) (2.24.0)\n",
      "Requirement already satisfied: biopython>=1.78 in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from Bio) (1.78)\n",
      "Requirement already satisfied: numpy<1.20 in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from Bio) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from requests->Bio) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from requests->Bio) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from requests->Bio) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\seyf_goumeida\\anaconda3\\lib\\site-packages (from requests->Bio) (1.25.11)\n",
      "Installing collected packages: Bio\n",
      "Successfully installed Bio-0.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "!pip install Bio\n",
    "import Bio\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3gFH3Z-uGdWm"
   },
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5poRtkuGdWn"
   },
   "source": [
    "#### specification du nombre de fichiers a lire : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B7dg7LQ8GdWo"
   },
   "outputs": [],
   "source": [
    "max_searchs = 50000\n",
    "term = \"coronavirus\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FYVl_5GdGdWp"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a06f5416d039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_searchs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntrez\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mesearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pubmed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mretstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntrez\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IdList'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\Entrez\\__init__.py\u001b[0m in \u001b[0;36mesearch\u001b[1;34m(db, term, **keywds)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"db\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"term\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcgi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\Entrez\\__init__.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(cgi, params, post, ecitmatch)\u001b[0m\n\u001b[0;32m    604\u001b[0m                 \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcgi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                 \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcgi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;31m# Reraise if the final try fails\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urllib.Request'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[0;32m    543\u001b[0m                                   '_open', req)\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1393\u001b[1;33m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[0;32m   1394\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[0;32m   1351\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1352\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[0;32m   1254\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1299\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\\r\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1422\u001b[0m                 \u001b[0mserver_hostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[0m\u001b[0;32m   1425\u001b[0m                                                   server_hostname=server_hostname)\n\u001b[0;32m   1426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1038\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Entrez.email = 'A.N.Other@example.com'\n",
    "ids=[[]]\n",
    "for i in range(0,max_searchs,100):\n",
    "\n",
    "  h = Entrez.esearch(db='pubmed', retmax=100,retstart=i, term=term)\n",
    "  result = Entrez.read(h)\n",
    "  ids.append(result['IdList'])\n",
    "h = Entrez.efetch(db='pubmed', id=ids, rettype='medline', retmode='json')\n",
    "records = Medline.parse(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00TwNPHxwKhU",
    "outputId": "0938b313-2b89-47ea-b72d-7ea6b755c9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMwUjqDr0u9H",
    "outputId": "70c44f8d-da44-4482-985c-e7aba180f3fd"
   },
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4ZLlyHTLPMu"
   },
   "outputs": [],
   "source": [
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "def append_records(records):\n",
    "  dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "  for idx, entry in enumerate(records):\n",
    "\n",
    "      if idx >100000:\n",
    "          break\n",
    "      try:\n",
    "          dict_['paper_id'].append(entry[\"PMID\"])\n",
    "      except Exception as e:\n",
    "          dict_['paper_id'].append(\"\")\n",
    "          \n",
    "      try:\n",
    "          dict_['abstract'].append(entry[\"AB\"])\n",
    "          # also create a column for the summary of abstract to be used in a plot\n",
    "          if len(entry[\"AB\"]) == 0: \n",
    "              # no abstract provided\n",
    "              dict_['abstract_summary'].append(\"Not provided.\")\n",
    "          elif len(entry[\"AB\"].split(' ')) > 100:\n",
    "              # abstract provided is too long for plot, take first 100 words append with ...\n",
    "              info = entry[\"AB\"].split(' ')[:100]\n",
    "              summary = get_breaks(' '.join(info), 40)\n",
    "              dict_['abstract_summary'].append(summary + \"...\")\n",
    "          else:\n",
    "              # abstract is short enough\n",
    "              summary = get_breaks(entry[\"AB\"], 40)\n",
    "              dict_['abstract_summary'].append(summary)\n",
    "      except Exception as e:\n",
    "          dict_['abstract'].append(\"\")  \n",
    "          dict_['abstract_summary'].append(\"Not provided.\")\n",
    "\n",
    "          \n",
    "      try:\n",
    "          dict_['body_text'].append(entry[\"AB\"])\n",
    "      except Exception as e:\n",
    "          dict_['body_text'].append(\"\")     \n",
    "      \n",
    "    \n",
    "      \n",
    "      \n",
    "      try:\n",
    "          # if more than one author\n",
    "          authors = entry['AU'][0].split(',')\n",
    "          if len(authors) > 2:\n",
    "              # if more than 2 authors, take them all with html tag breaks in between\n",
    "              dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
    "          else:\n",
    "              # authors will fit in plot\n",
    "              dict_['authors'].append(\". \".join(authors))\n",
    "      except Exception as e:\n",
    "          # if only one author - or Null valie\n",
    "          try : \n",
    "              dict_['authors'].append(entry['AU'][0])\n",
    "          except Exception as e:\n",
    "              dict_['authors'].append(\"\")\n",
    "\n",
    "\n",
    "      \n",
    "      # add the title information, add breaks when needed\n",
    "      try:\n",
    "          title = get_breaks(entry['TI'][0], 40)\n",
    "          dict_['title'].append(title)\n",
    "      # if title was not provided\n",
    "      except Exception as e:\n",
    "          try:\n",
    "              dict_['title'].append(entry['TI'][0])\n",
    "          except Exception as e:\n",
    "              dict_['title'].append(\"\")\n",
    "      \n",
    "      # add the journal information\n",
    "      dict_['journal'].append(entry['PT'][0])\n",
    "\n",
    "\n",
    "  return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8peedbYLaus"
   },
   "outputs": [],
   "source": [
    "dict_1 = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for i in range(1,len(ids)):\n",
    "  h = Entrez.efetch(db='pubmed', id=ids[i], rettype='medline', retmode='json')\n",
    "  records = Medline.parse(h)\n",
    "\n",
    "  token = append_records(records)\n",
    "  dict_1[\"paper_id\"] = dict_1[\"paper_id\"]+token[\"paper_id\"]\n",
    "  dict_1[\"abstract\"] = dict_1[\"abstract\"] + token[\"abstract\"]\n",
    "  dict_1[\"body_text\"] =  dict_1[\"body_text\"] +token[\"body_text\"]\n",
    "  dict_1[\"authors\"] = dict_1[\"authors\"] + token[\"authors\"]\n",
    "  dict_1[\"title\"] =dict_1[\"title\"] + token[\"title\"]\n",
    "  dict_1[\"journal\"] =dict_1[\"journal\"] +token[\"journal\"]\n",
    "  dict_1[\"abstract_summary\"]=dict_1[\"abstract_summary\"]+token[\"abstract_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX3e5AC_Ldkk",
    "outputId": "8714b31c-3d98-40fa-ac63-c7f41d92e0a9"
   },
   "outputs": [],
   "source": [
    "df_covid = pd.DataFrame(dict_1, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMa3rqKRyd6J",
    "outputId": "faef5218-01d4-4226-b5b7-44a7d32d4177"
   },
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyhzey_1GdWq"
   },
   "source": [
    "## Some feature engineering\n",
    "Adding word count columns for both abstract and body_text can be useful parameters later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "xqMfwG5pGdWr",
    "outputId": "5e38093c-aff6-4d39-e985-58ea14c0d7d2"
   },
   "outputs": [],
   "source": [
    "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n",
    "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\n",
    "df_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MO7Hog0hGdWr",
    "outputId": "5722f8af-080d-4de3-ddea-fe80dd68be2d"
   },
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTiWLFCgGdWs",
    "outputId": "7217e43b-416c-4ce4-85de-e4d2998acc22"
   },
   "outputs": [],
   "source": [
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4vZi8C1GdWs"
   },
   "source": [
    "## Handle Possible Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d027A619GdWt"
   },
   "source": [
    "When we look at the unique values above, we can see that tehre are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n",
    "\n",
    "(Thank you Desmond Yeoh for recommending the below approach on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFhtgIDDGdWy",
    "outputId": "dca48617-c551-4d86-900c-c9dce24a8c68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 71247\n",
       "unique                                                71247\n",
       "top       The recently emerged novel coronavirus pneumon...\n",
       "freq                                                      1\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ec-Q6244GdWz",
    "outputId": "6eb5d8f4-5bb6-4ddd-90fe-f7cd77be5cb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 71247\n",
       "unique                                                71247\n",
       "top       The recently emerged novel coronavirus pneumon...\n",
       "freq                                                      1\n",
       "Name: body_text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQwDVNJUGdW0"
   },
   "source": [
    "It looks like we didn't have duplicates. Instead, it was articles without Abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftrYJq7xGdW0"
   },
   "source": [
    "## Take a Look at the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "hSxoIZp6GdW1",
    "outputId": "429465f1-c810-42ee-e215-4f72217aed9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract_summary</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33979833</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Finkel Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Severe acute respiratory syndrome&lt;br&gt;coronavi...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33979817</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>Chabal LO</td>\n",
       "      <td>P</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15&lt;br&gt;recom...</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33979791</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Ma Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus&lt;br&gt;D...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33979779</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>O'Kane SM</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of&lt;b...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33979771</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>Barzegar M</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19)&lt;...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  ... body_unique_words\n",
       "0  33979833  ...               125\n",
       "1  33979817  ...                85\n",
       "2  33979791  ...               133\n",
       "3  33979779  ...               130\n",
       "4  33979771  ...                78\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2_a1G7JGdW1"
   },
   "source": [
    "In the majority of this notebook we will be working with **body_text** <br>\n",
    "Links to the papers will be generated using **doi** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "nrL1T8XrGdW2",
    "outputId": "9a1226d1-2363-4604-8f4e-5ee9fbcb733e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>71247.000000</td>\n",
       "      <td>71247.000000</td>\n",
       "      <td>71247.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>207.187545</td>\n",
       "      <td>207.187545</td>\n",
       "      <td>134.080270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>89.188360</td>\n",
       "      <td>89.188360</td>\n",
       "      <td>47.336653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>149.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>135.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1899.000000</td>\n",
       "      <td>1899.000000</td>\n",
       "      <td>894.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       abstract_word_count  body_word_count  body_unique_words\n",
       "count         71247.000000     71247.000000       71247.000000\n",
       "mean            207.187545       207.187545         134.080270\n",
       "std              89.188360        89.188360          47.336653\n",
       "min               0.000000         0.000000           0.000000\n",
       "25%             149.000000       149.000000         104.000000\n",
       "50%             205.000000       205.000000         135.000000\n",
       "75%             254.000000       254.000000         162.000000\n",
       "max            1899.000000      1899.000000         894.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyanCizrGdW2"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQlh6HQWGdW2"
   },
   "source": [
    "### Réduction de la taille du dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UsE-eeIFGdW3"
   },
   "outputs": [],
   "source": [
    "\n",
    "#df = df_covid.sample(4000, random_state=42)\n",
    "df = df_covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXlRaj-3GdW3"
   },
   "source": [
    "### Suppression des valeurs null :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPzUrJjrGdW3"
   },
   "source": [
    "Now that we have our dataset loaded, we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu-44fZZGdW3",
    "outputId": "ed0c8b4a-88c1-4878-c4f7-6242dcef0e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 71247 entries, 0 to 99999\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   paper_id             71247 non-null  object\n",
      " 1   abstract             71247 non-null  object\n",
      " 2   body_text            71247 non-null  object\n",
      " 3   authors              71247 non-null  object\n",
      " 4   title                71247 non-null  object\n",
      " 5   journal              71247 non-null  object\n",
      " 6   abstract_summary     71247 non-null  object\n",
      " 7   abstract_word_count  71247 non-null  int64 \n",
      " 8   body_word_count      71247 non-null  int64 \n",
      " 9   body_unique_words    71247 non-null  int64 \n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFMzbLH4GdW4"
   },
   "source": [
    "### Handling multiple languages\n",
    "Next we are going to determine the language of each paper in the dataframe. Not all of the sources are English and the language needs to be identified so that we know how handle these instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcE7FbU9GdW4",
    "outputId": "0cdcb245-b287-4511-fa31-afd30c584449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 4.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993223 sha256=e76fbbf263436032c35505e33be2470ec44955bea66657e6b293bfa520ac34c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71247/71247 [06:49<00:00, 173.92it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "import tqdm\n",
    "!pip install langdetect\n",
    "import langdetect\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# hold label - language\n",
    "languages = []\n",
    "\n",
    "# go through each text\n",
    "for ii in tqdm(range(0,len(df))):\n",
    "    # split by space into list, take the first x intex, join with space\n",
    "    text = df.iloc[ii]['body_text'].split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # what!! :( let's see if we can find any text in abstract...\n",
    "        except Exception as e:\n",
    "            \n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(df.iloc[ii]['abstract_summary'])\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "    \n",
    "    # get the language    \n",
    "    languages.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NemBurd8GdW5",
    "outputId": "a4c2e457-0b0e-4146-b86d-f1dc22953df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 71247\n",
      "\n",
      "{'ca': 5,\n",
      " 'da': 1,\n",
      " 'de': 5,\n",
      " 'en': 71179,\n",
      " 'es': 16,\n",
      " 'fr': 11,\n",
      " 'hu': 17,\n",
      " 'id': 2,\n",
      " 'it': 6,\n",
      " 'no': 1,\n",
      " 'ro': 1,\n",
      " 'unknown': 2,\n",
      " 'vi': 1}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "languages_dict = {}\n",
    "for lang in set(languages):\n",
    "    languages_dict[lang] = languages.count(lang)\n",
    "    \n",
    "print(\"Total: {}\\n\".format(len(languages)))\n",
    "pprint(languages_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pU5nSAiPGdW5"
   },
   "source": [
    "Lets take a look at the language distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RcRjv3ayqhOl",
    "outputId": "22788c90-cb3a-4c57-8da1-9050ea46148f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0R5MVtGdGdW5",
    "outputId": "b09c9325-1769-4a38-b7cb-fb52af8ebdad"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-dcebb1cec099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribution of Languages in Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3118\u001b[0m         \"\"\"\n\u001b[1;32m   3119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3120\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3121\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3767\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3768\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3770\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         raise ValueError(\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (71247) does not match length of index (71179)"
     ]
    }
   ],
   "source": [
    "df['language'] = languages\n",
    "plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
    "plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
    "plt.title(\"Distribution of Languages in Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1y8e4mXGdW6"
   },
   "source": [
    "\n",
    "We will be dropping any language that is not English. Attempting to translate foreign texts gave the following problems:\n",
    "\n",
    "1. API calls were limited\n",
    "\n",
    "2. Translating the language may not carry over the true semantic meaning of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VaqWVbFGdW6",
    "outputId": "d68e2a6c-1b5e-4360-83fb-a31e97fcec5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 71179 entries, 0 to 99999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   paper_id                  71179 non-null  object\n",
      " 1   abstract                  71179 non-null  object\n",
      " 2   body_text                 71179 non-null  object\n",
      " 3   authors                   71179 non-null  object\n",
      " 4   title                     71179 non-null  object\n",
      " 5   journal                   71179 non-null  object\n",
      " 6   abstract_summary          71179 non-null  object\n",
      " 7   abstract_word_count       71179 non-null  int64 \n",
      " 8   body_word_count           71179 non-null  int64 \n",
      " 9   body_unique_words         71179 non-null  int64 \n",
      " 10  language                  71179 non-null  object\n",
      " 11  processed_text            71179 non-null  object\n",
      " 12  text_wo_stopfreq          71179 non-null  object\n",
      " 13  text_lemmatized           71179 non-null  object\n",
      " 14  text_lemmatized_no_url    71179 non-null  object\n",
      " 15  text_lemmatized_url_html  71179 non-null  object\n",
      "dtypes: int64(3), object(13)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df[df['language'] == 'en'] \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "cnysIJgzGdW6"
   },
   "outputs": [],
   "source": [
    "#Download the spacy bio parser\n",
    "\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "  !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRxDvLIFGdW7",
    "outputId": "2d32eee1-b3fa-4681-edfd-cc4f1aa4484d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "UGhyIiXsGdW7"
   },
   "outputs": [],
   "source": [
    "#NLP \n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg  # model downloaded in previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z6kHFa5GdW7"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Part of the preprocessing will be finding and removing stopwords (common words that will act as noise in the clustering step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr5l1GxpGdW7",
    "outputId": "0bf66852-4354-48dd-d92c-6bde7fabd0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hence', 'someone', 'some', 'go', '’m', 'must', 'a', 'anything', 'to', 'me', 'keep', 'give', 'whatever', 'cannot', 'under', 'all', 'she', 'its', 'them', 'not', 'into', 'quite', 'bottom', 'ours', 'using', 'many', 'whose', 'at', 'ca', \"'re\", 'somehow', 'in', '‘s', '‘ll', \"n't\", 'over', 'any', 'him', '’d', 'often', 'sometimes', 'seems', 'sometime', 'used', 'after', 'perhaps', 'sixty', 'whereupon', 'he', 'there', 'however', 'side', 'across', 'during', 'move', 'here', 'indeed', 'least', 'other', 'anyhow', 'should', 'although', 'elsewhere', 're', '‘re', 'had', 'itself', 'thereupon', '’ll', 'another', 'where', 'my', 'twenty', 'wherein', 'still', 'anywhere', 'have', 'please', 'why', 'none', 'two', 'whether', 'does', 'something', 'therefore', 'done', 'most', 'others', 'when', 'nor', 'everywhere', 'whither', 'per', 'thus', 'did', 'off', 'nevertheless', 'forty', 'various', 'call', 'am', 'regarding', 'through', 'n‘t', 'from', 'more', 'do', 'until', 'next', 'or', 'via', 'beyond', 'so', 'are', 'everyone', '‘ve', 'upon', 'within', 'latterly', 'moreover', 'themselves', 'yourselves', 'than', 'full', 'that', 'first', 'very', 'alone', 'again', 'below', 'seemed', 'were', 'with', \"'ve\", '‘m', 'became', 'eight', 'namely', 'never', 'just', 'five', 'whereafter', '’ve', 'behind', 'each', 'afterwards', \"'ll\", 'along', 'us', 'therein', 'for', 'become', 'it', 'among', 'beforehand', 'we', 'the', 'how', 'throughout', 'one', 'be', 'their', 'will', 'anyway', 'being', 'himself', 'neither', 'whom', 'yet', 'no', 'else', 'six', \"'d\", 'could', 'doing', 'toward', 'both', 'though', 'otherwise', 'herself', 'before', 'unless', 'whenever', 'twelve', 'empty', 'these', 'yours', 'thereafter', 'is', 'and', 'as', 'name', 'take', 'this', 'seeming', 'almost', 'even', 'about', 'front', 'becoming', 'on', 'meanwhile', 'been', 'whole', 'latter', 'while', 'might', 'top', 'whoever', '’re', 'whereby', 'nowhere', 'nine', 'i', 'always', 'since', 'due', 'hereafter', 'around', 'n’t', 'several', 'formerly', '‘d', 'amount', 'myself', 'either', 'rather', 'thru', 'four', 'of', 'everything', 'thereby', 'an', 'herein', 'what', 'you', 'thence', 'becomes', 'between', 'part', 'somewhere', 'mine', 'wherever', 'her', \"'m\", 'also', 'without', 'they', 'seem', 'fifteen', 'made', 'towards', 'except', 'serious', 'who', 'onto', 'yourself', 'once', 'eleven', 'has', 'because', 'noone', 'really', 'too', 'your', 'make', 'out', 'would', 'was', 'ourselves', 'besides', 'whereas', 'show', 'hundred', 'last', 'few', 'see', 'down', 'well', 'hereupon', 'ever', 'but', 'such', 'our', 'say', 'which', 'get', 'nothing', 'own', 'former', 'anyone', 'together', 'then', 'less', 'can', '’s', 'mostly', 'much', 'amongst', 'every', 'above', 'beside', 'back', 'may', 'if', 'hereby', 'by', 'already', 'only', 'same', 'further', \"'s\", 'third', 'enough', 'his', 'up', 'now', 'put', 'three', 'whence', 'against', 'those', 'nobody', 'fifty', 'ten', 'hers']\n",
      "-----------\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords)\n",
    "print(\"-----------\")\n",
    "\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGzgeg9dGdW8"
   },
   "source": [
    "Now the above stopwords are used in everyday english text. Research papers will often frequently use words that don't actually contribute to the meaning and are not considered everyday stopwords.\n",
    "\n",
    "Thank you Daniel Wolffram for the idea.\n",
    "#### Cite: [Custom Stop Words | Topic Modeling: Finding Related Articles](https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "jIWi1cUBGdW8"
   },
   "outputs": [],
   "source": [
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n",
    "]\n",
    "\n",
    "for w in custom_stop_words:\n",
    "    if w not in stopwords:\n",
    "        stopwords.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vqksra4AGdW8"
   },
   "source": [
    "### Next lets create a function that will process the text data for us. \n",
    "For this purpose we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "sUGMFC2hGdW9"
   },
   "outputs": [],
   "source": [
    "# Parser\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word.text not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59xOir8bGdW9"
   },
   "source": [
    "Applying the text-processing function on the **body_text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ySfURtBsGdW9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#//////////////////////////////////////////////////////////////\n",
    "#//////////////////////////////////////////////////////////////\n",
    "#//////////////////////////////////////////////////////////////\n",
    "#c'est dans cette partie la que ça bug , ça prend un temps énorme et on a pas eu de résultat  \n",
    "#//////////////////////////////////////////////////////////////\n",
    "#//////////////////////////////////////////////////////////////\n",
    "#//////////////////////////////////////////////////////////////\n",
    "#tqdm.pandas()\n",
    "#df[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "VXz-7XXYGdW9"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = en_core_web_lg.load()\n",
    "def spacy_process(texte):\n",
    "    tmp = []\n",
    "    tmp.append(texte)\n",
    "    for lt in tmp:\n",
    "        mytokens = nlp(lt)\n",
    "        mytokens2 = [word.lemma_.lower().strip() for word in mytokens if word.pos_ != \"PUNCT\" and word.text not in stopwords]\n",
    "        mytokens2 = \" \".join([i for i in mytokens2])\n",
    "    return mytokens2\n",
    "df[\"processed_text\"] = df[\"body_text\"].apply(spacy_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUa3hW_rGdW-"
   },
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4KDJ0KqGdW-",
    "outputId": "083e10b7-26bb-488f-bcde-e9eeff609ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaccination\n",
      "vaccination\n",
      "vaccinate\n",
      "vaccinate\n"
     ]
    }
   ],
   "source": [
    "test1 = spacy_process(\"vaccinations\")\n",
    "test2 = spacy_process(\"vaccination\")\n",
    "test3 = spacy_process(\"vaccinating\")\n",
    "test4 = spacy_process(\"vaccinated\")\n",
    "print(test1)\n",
    "print(test2)\n",
    "print(test3)\n",
    "print(test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "Rn05paVyGdW-",
    "outputId": "c8c97c88-9313-45bc-96c0-f9b2383a55f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract_summary</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "      <th>language</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_lemmatized_no_url</th>\n",
       "      <th>text_lemmatized_url_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33979833</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Finkel Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Severe acute respiratory syndrome&lt;br&gt;coronavi...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>125</td>\n",
       "      <td>en</td>\n",
       "      <td>severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33979817</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>Chabal LO</td>\n",
       "      <td>P</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15&lt;br&gt;recom...</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>85</td>\n",
       "      <td>en</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33979791</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Ma Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus&lt;br&gt;D...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>133</td>\n",
       "      <td>en</td>\n",
       "      <td>segment lesion region coronavirus disease 2019...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33979779</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>O'Kane SM</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of&lt;b...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>130</td>\n",
       "      <td>en</td>\n",
       "      <td>background to suppress transmission coronaviru...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33979771</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>Barzegar M</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19)&lt;...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>78</td>\n",
       "      <td>en</td>\n",
       "      <td>the effect coronavirus disease covid-19 risk r...</td>\n",
       "      <td>effect relapse multiple sclerosis ms unknown r...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  ...                           text_lemmatized_url_html\n",
       "0  33979833  ...  acute syndrome 2 cause ongoing 19 pandemic(1 c...\n",
       "1  33979817  ...  general purpose to introduce 15 recommendation...\n",
       "2  33979791  ...  segment lesion region compute tomography ct im...\n",
       "3  33979779  ...  background to suppress transmission government...\n",
       "4  33979771  ...  effect relapse multiple sclerosis m unknown re...\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGaZ-EIcGdW-",
    "outputId": "2dcb9bc5-311b-4f4e-918e-c197e3cfef10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "h\n",
      "e\n",
      "\n",
      "c\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "-\n",
      "1\n",
      "9\n",
      "\n",
      "p\n",
      "a\n",
      "n\n",
      "d\n",
      "e\n",
      "m\n",
      "i\n",
      "c\n",
      "\n",
      "p\n",
      "o\n",
      "s\n",
      "e\n",
      "s\n",
      "\n",
      "n\n",
      "e\n",
      "w\n",
      "\n",
      "c\n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      "e\n",
      "n\n",
      "g\n",
      "e\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "tmp = \"The COVID-19 pandemic poses new challenges\"\n",
    "for lt in tmp:\n",
    "    mytokens = nlp(lt)\n",
    "    mytokens2 = [word.lemma_.lower().strip() for word in mytokens]\n",
    "    mytokens2 = \" \".join([i for i in mytokens2])\n",
    "    print(mytokens2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZye4CoHGdXA",
    "outputId": "5bf7ce2d-7a30-4ae4-8f3d-0482817cd8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERAL PURPOSE: To introduce the 15 recommendations of the International Ostomy Guideline (IOG) 2020, covering the four key arenas of education, holistic aspects, and pre- and postoperative care; and to summarize key concepts for clinicians to customize for translation into their practice. TARGET AUDIENCE: This continuing education activity is intended for physicians, physician assistants, nurse practitioners, and nurses with an interest in skin and wound care. LEARNING OBJECTIVES/OUTCOMES: After participating in this educational activity, the participant will:1. Analyze supporting evidence for the education recommendations in the IOG 2020.2. Identify a benefit of the International Charter of Ostomate Rights.3. Distinguish concepts related to pre- and postoperative ostomy-related care.4. Select a potential barrier to IOG 2020 guideline implementation.\n",
      "\n",
      "general purpose to introduce 15 recommendation international ostomy guideline iog 2020 cover key arena education holistic aspect pre- postoperative care summarize key concept clinician customize translation practice target audience this continue education activity intend physician physician assistant nurse practitioner nurse interest skin wound care learning objective / outcomes after participate educational activity participant will:1 analyze support evidence education recommendation iog 2020.2 identify benefit international charter ostomate rights.3 distinguish concept relate pre- postoperative ostomy relate care.4 select potential barrier iog 2020 guideline implementation\n",
      "\n",
      "----------------------------------------------------------\n",
      "Segmenting lesion regions of Coronavirus Disease 2019 (COVID-19) from computed tomography (CT) images is a challenge owing to COVID-19 lesions characterized by high variation, low contrast between infection lesions and around normal tissues, and blurred boundaries of infections. Moreover, a shortage of available CT dataset hinders deep learning techniques applying to tackling COVID-19. To address these issues, we propose a deep learning-based approach known as PPM-Unet to segmenting COVID-19 lesions from CT images. Our method improves an Unet by adopting pyramid pooling modules instead of the conventional skip connection and then enhances the representation of the neural network by aiding the global attention mechanism. We first pre-train PPM-Unet on COVID-19 dataset of pseudo labels containing1600 samples producing a coarse model. Then we fine-tune the coarse PPM-Unet on the standard COVID-19 dataset consisting of 100 pairs of samples to achieve a fine PPM-Unet. Qualitative and quantitative results demonstrate that our method can accurately segment COVID-19 infection regions from CT images, and achieve higher performance than other state-of-the-art segmentation models in this study. It offers a promising tool to lay a foundation for quantitatively detecting COVID-19 lesions.\n",
      "\n",
      "segment lesion region coronavirus disease 2019 covid-19 compute tomography ct image challenge owe covid-19 lesion characterize high variation low contrast infection lesion normal tissue blur boundary infection moreover shortage available ct dataset hinder deep learning technique apply tackle covid-19 to address issue propose deep learning base approach know ppm unet segment covid-19 lesion ct image our method improve unet adopt pyramid pooling module instead conventional skip connection enhance representation neural network aid global attention mechanism we pre - train ppm unet covid-19 dataset pseudo label containing1600 sample produce coarse model then fine tune coarse ppm unet standard covid-19 dataset consist 100 pair sample achieve fine ppm unet qualitative quantitative result demonstrate method accurately segment covid-19 infection region ct image achieve high performance state art segmentation model study it offer promising tool lay foundation quantitatively detect covid-19 lesion\n",
      "\n",
      "----------------------------------------------------------\n",
      "BACKGROUND: To suppress the transmission of coronavirus, many governments, including that of the island of Ireland, implemented a societal lockdown, which included school closures, limits on social gatherings, and time outdoors. This study aimed to evaluate changes in physical activity (PA), mental health, sleep, and social media use among adolescent girls during lockdown. METHODS: 281 female pupils (12-14 y) taking part in the ongoing Walking In Schools study on the island of Ireland self-reported PA, mental health, sleep, and social media use before (September-October 2019) and during lockdown (May-June 2020), via questionnaires. These were supplemented with open-ended structured interviews conducted with 16 girls during lockdown. RESULTS: During the period of lockdown and school closures, pupils tried new forms of PA and undertook PA with family, but there was no significant change in self-reported PA. There was a decline in health-related quality of life and motivation for exercise; however, self-efficacy for walking and happiness with appearance increased. There was no change in sleep quality or social media usage. CONCLUSIONS: Despite the many challenges that schools face as they reopen, there is a need to continue to prioritize PA and motivation for exercise to support health and well-being in adolescent girls.\n",
      "\n",
      "background to suppress transmission coronavirus government include island ireland implement societal lockdown include school closure limit social gathering time outdoors this study aim evaluate change physical activity pa mental health sleep social medium use adolescent girl lockdown method 281 female pupil 12 - 14 y take ongoing walking in schools study island ireland self report pa mental health sleep social medium use september october 2019 lockdown may june 2020 questionnaire these supplement open end structured interview conduct 16 girl lockdown result during period lockdown school closure pupil try new form pa undertake pa family significant change self report pa there decline health relate quality life motivation exercise self efficacy walking happiness appearance increase there change sleep quality social medium usage conclusion despite challenge school face reopen need continue prioritize pa motivation exercise support health adolescent girl\n",
      "\n",
      "----------------------------------------------------------\n",
      "The effect of coronavirus disease (COVID-19) on the risk of relapse in multiple sclerosis (MS) have been unknown. In this retrospective study of 41 relapsing-remitting MS patients, number of relapses in pre-defined at risk-period (ARP) was compared with the previous two years. During the previous two years, a total of 32 attacks was reported, which 5 (15.6%) were during the at-risk period. After adjusting for age and sex, there was an increased risk of attack during ARP compared to the previous two years (RR: 2.566, 95%CI: 1.075-6.124, P=0.034). Our preliminary study suggested that COVID-19 can trigger exacerbation of MS.\n",
      "\n",
      "the effect coronavirus disease covid-19 risk relapse multiple sclerosis ms unknown in retrospective study 41 relapse remit ms patient number relapse pre - define risk period arp compare previous year during previous year total 32 attack report 5 15.6 % risk period after adjust age sex increased risk attack arp compare previous year rr 2.566 95%ci 1.075 6.124 p=0.034 our preliminary study suggest covid-19 trigger exacerbation ms\n",
      "\n",
      "----------------------------------------------------------\n",
      "Coronavirus disease 2019 (COVID-19) patients have manifested a variety of neurological complications, and there is still much to reveal regarding the neurotropism of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Human stem cell-derived brain organoids offer a valuable in vitro approach to study the cellular effects of SARS-CoV-2 on the brain. Here we used human embryonic stem cell-derived cortical organoids to investigate whether SARS-CoV-2 could infect brain tissue in vitro and found that cortical organoids could be infected at low viral titers and within 6 h. Importantly, we show that glial cells and cells of the choroid plexus were preferentially targeted in our model, but not neurons. Interestingly, we also found expression of angiotensin-converting enzyme 2 in SARS-CoV-2 infected cells; however, viral replication and cell death involving DNA fragmentation does not occur. We believe that our model is a tractable platform to study the cellular effects of SARS-CoV-2 infection in brain tissue.\n",
      "\n",
      "coronavirus disease 2019 covid-19 patient manifest variety neurological complication reveal neurotropism severe acute respiratory syndrome coronavirus 2 sars cov-2 human stem cell derive brain organoid offer valuable vitro approach study cellular effect sars cov-2 brain here human embryonic stem cell derive cortical organoid investigate sars cov-2 infect brain tissue vitro find cortical organoid infect low viral titer 6 h. importantly glial cell cell choroid plexus preferentially target model neuron interestingly find expression angiotensin convert enzyme 2 sars cov-2 infect cell viral replication cell death involve dna fragmentation occur we believe model tractable platform study cellular effect sars cov-2 infection brain tissue\n",
      "\n",
      "----------------------------------------------------------\n",
      "We detected severe acute respiratory syndrome coronavirus 2 in an otherwise healthy poodle living with 4 family members who had coronavirus disease. We observed antibodies in serum samples taken from the dog, indicating seroconversion. Full-length genome sequencing showed that the canine and human viruses were identical, suggesting human-to-animal transmission.\n",
      "\n",
      "we detect severe acute respiratory syndrome coronavirus 2 healthy poodle live 4 family member coronavirus disease we observe antibody serum sample take dog indicate seroconversion full length genome sequencing show canine human virus identical suggest human animal transmission\n",
      "\n",
      "----------------------------------------------------------\n",
      "We describe response measures to an outbreak involving 128 (33.4%) coronavirus disease cases (46.1% asymptomatic) among 383 persons onboard a passenger ship. Multivariate analysis indicated that dining in certain rooms and bar areas, nationality, working department (for crew members), and quarantining onboard the ship were significantly associated with infection.\n",
      "\n",
      "we describe response measure outbreak involve 128 33.4 % coronavirus disease case 46.1 % asymptomatic 383 person onboard passenger ship multivariate analysis indicate dining certain room bar area nationality working department crew member quarantine onboard ship significantly associate infection\n",
      "\n",
      "----------------------------------------------------------\n",
      "Worksites with on-site operations have experienced coronavirus disease (COVID-19) outbreaks. We analyzed data for 698 nonresidential, nonhealthcare worksite COVID-19 outbreaks investigated in Los Angeles County, California, USA, during March 19, 2020September 30, 2020, by using North American Industry Classification System sectors and subsectors. Nearly 60% of these outbreaks occurred in 3 sectors: manufacturing (n = 184, 26.4%), retail trade (n = 137, 19.6%), and transportation and warehousing (n = 73, 10.5%). The largest number of outbreaks and largest number and highest incidence rate of outbreak-associated cases occurred in manufacturing. Furthermore, 7 of the 10 industry subsectors with the highest incidence rates were within manufacturing. Approximately 70% of outbreak-associated case-patients reported Hispanic ethnicity. Facilities employing more on-site staff had larger and longer outbreaks. Identification of highly affected industry sectors and subsectors is necessary for targeted public health planning, outreach, and response, including ensuring vaccine access, to reduce burden of COVID-19 in vulnerable workers.\n",
      "\n",
      "worksite site operation experience coronavirus disease covid-19 outbreak we analyze datum 698 nonresidential nonhealthcare worksite covid-19 outbreak investigate los angeles county california usa march 19 2020september 30 2020 north american industry classification system sector subsector nearly 60 % outbreak occur 3 sector manufacturing n = 184 26.4 % retail trade n = 137 19.6 % transportation warehousing n = 73 10.5 % the large number outbreak large number high incidence rate outbreak associate case occur manufacturing furthermore 7 10 industry subsector high incidence rate manufacturing approximately 70 % outbreak associate case patient report hispanic ethnicity facility employ site staff large long outbreak identification highly affected industry sector subsector necessary target public health planning outreach response include ensure vaccine access reduce burden covid-19 vulnerable worker\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "processed_text = df[\"processed_text\"]\n",
    "body_text = df[\"body_text\"]\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(body_text[i])\n",
    "    print(\"\")\n",
    "    print(processed_text[i])\n",
    "    print(\"\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BGXvkPuGdXB"
   },
   "source": [
    "### Removal of Frequent words\n",
    "So this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJLC9z5dGdXC",
    "outputId": "794dc8f8-4c71-4b89-a48d-e0d5a4482132"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid-19', 184585),\n",
       " ('patient', 133398),\n",
       " ('%', 124652),\n",
       " ('the', 114428),\n",
       " ('sars', 80807),\n",
       " ('disease', 73047),\n",
       " ('-', 72985),\n",
       " ('cov-2', 66397),\n",
       " ('coronavirus', 65931),\n",
       " ('study', 63510),\n",
       " ('pandemic', 57625),\n",
       " ('infection', 55742),\n",
       " ('result', 48282),\n",
       " ('health', 47434),\n",
       " ('case', 43246),\n",
       " ('virus', 41284),\n",
       " ('respiratory', 40928),\n",
       " ('high', 40597),\n",
       " ('severe', 39209),\n",
       " ('we', 38410),\n",
       " ('in', 36998),\n",
       " ('clinical', 35747),\n",
       " ('2019', 34957),\n",
       " ('risk', 33566),\n",
       " ('include', 33119),\n",
       " ('care', 32170),\n",
       " ('2020', 29918),\n",
       " ('this', 29180),\n",
       " ('method', 29076),\n",
       " ('report', 28970),\n",
       " ('acute', 28536),\n",
       " ('/', 27835),\n",
       " ('=', 27393),\n",
       " ('2', 27364),\n",
       " ('treatment', 26340),\n",
       " ('increase', 26316),\n",
       " ('datum', 26228),\n",
       " ('syndrome', 25728),\n",
       " ('a', 25009),\n",
       " ('cell', 24766)]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"processed_text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rgoqRLVGdXD",
    "outputId": "087c508a-19e3-4874-d51c-cb9e8a596a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(30)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"text_wo_stopfreq\"] = df[\"processed_text\"].apply(lambda text: remove_freqwords(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Knx3BL73GdXD",
    "outputId": "357f2e6d-6f8b-4188-802c-dc0a1ff9552a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Severe acute respiratory syndrome coronavirus ...\n",
       "1        GENERAL PURPOSE: To introduce the 15 recommend...\n",
       "2        Segmenting lesion regions of Coronavirus Disea...\n",
       "3        BACKGROUND: To suppress the transmission of co...\n",
       "4        The effect of coronavirus disease (COVID-19) o...\n",
       "                               ...                        \n",
       "99995    From the mid-1960s onwards, it was believed th...\n",
       "99996    Mouse hepatitis virus (MHV) causes encephaliti...\n",
       "99997    BACKGROUND: In March of 2003, an outbreak of S...\n",
       "99998    The severe acute respiratory syndrome coronavi...\n",
       "99999    Both the threat of bioterrorism and the natura...\n",
       "Name: body_text, Length: 71179, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_W4W_JKGdXE",
    "outputId": "d9c03bd2-b557-486f-bd6f-d54c6a1417ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        acute syndrome 2 cause ongoing 19 pandemic(1 c...\n",
       "1        general purpose to introduce 15 recommendation...\n",
       "2        segment lesion region compute tomography ct im...\n",
       "3        background to suppress transmission government...\n",
       "4        effect relapse multiple sclerosis ms unknown r...\n",
       "                               ...                        \n",
       "99995    from mid-1960 onwards believe human specie inf...\n",
       "99996    mouse hepatitis mhv cause encephalitis demyeli...\n",
       "99997    background march 2003 outbreak acute syndrome ...\n",
       "99998    acute syndrome cov nucleocapsid protein np pre...\n",
       "99999    both threat bioterrorism natural emergence con...\n",
       "Name: text_wo_stopfreq, Length: 71179, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_wo_stopfreq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5wMKU0uGdXF",
    "outputId": "d214ac7b-8d73-4624-ce6d-45a4ce56eab1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        severe acute respiratory syndrome coronavirus ...\n",
       "1        general purpose to introduce 15 recommendation...\n",
       "2        segment lesion region coronavirus disease 2019...\n",
       "3        background to suppress transmission coronaviru...\n",
       "4        the effect coronavirus disease covid-19 risk r...\n",
       "                               ...                        \n",
       "99995    from mid-1960 onwards believe human coronaviru...\n",
       "99996    mouse hepatitis virus mhv cause encephalitis d...\n",
       "99997    background in march 2003 outbreak severe acute...\n",
       "99998    the severe acute respiratory syndrome coronavi...\n",
       "99999    both threat bioterrorism natural emergence con...\n",
       "Name: processed_text, Length: 71179, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrP6BUuHGdXF"
   },
   "source": [
    "### Removal of Rare words\n",
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Kpq7VQNpGdXF"
   },
   "outputs": [],
   "source": [
    "#cnt.most_common()[:-500-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7bKfD-Y1GdXG"
   },
   "outputs": [],
   "source": [
    "# n_rare_words = 10\n",
    "# RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "# def remove_rarewords(text):\n",
    "#     \"\"\"custom function to remove the rare words\"\"\"\n",
    "#     return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "# df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0FbykEXGdXG"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm9I-FXuGdXG"
   },
   "source": [
    "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf3sCFnZGdXH",
    "outputId": "b2580851-b132-4ef0-d1ca-137b4c659cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "pPVDdQwkGdXH",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLrGL6OGGdXH"
   },
   "source": [
    "Now let us redo the lemmatization process for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uW_fDiFGdXI",
    "outputId": "9453fd98-5d2d-4d48-f754-3aaaf04bf271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-UWVmP0GdXI",
    "outputId": "5b635081-65ca-44a3-d232-af719d5e82e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word, pos in pos_tagged_text])\n",
    "\n",
    "df[\"text_lemmatized\"] = df[\"text_wo_stopfreq\"].apply(lambda text: lemmatize_words(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JgclwSTTGdXI",
    "outputId": "27921cba-c5cf-4fc5-d674-946e55ce9487"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract_summary</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "      <th>language</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33979833</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Finkel Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Severe acute respiratory syndrome&lt;br&gt;coronavi...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>125</td>\n",
       "      <td>en</td>\n",
       "      <td>severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33979817</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>Chabal LO</td>\n",
       "      <td>P</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15&lt;br&gt;recom...</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>85</td>\n",
       "      <td>en</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33979791</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Ma Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus&lt;br&gt;D...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>133</td>\n",
       "      <td>en</td>\n",
       "      <td>segment lesion region coronavirus disease 2019...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33979779</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>O'Kane SM</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of&lt;b...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>130</td>\n",
       "      <td>en</td>\n",
       "      <td>background to suppress transmission coronaviru...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33979771</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>Barzegar M</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19)&lt;...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>78</td>\n",
       "      <td>en</td>\n",
       "      <td>the effect coronavirus disease covid-19 risk r...</td>\n",
       "      <td>effect relapse multiple sclerosis ms unknown r...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  ...                                    text_lemmatized\n",
       "0  33979833  ...  acute syndrome 2 cause ongoing 19 pandemic(1 c...\n",
       "1  33979817  ...  general purpose to introduce 15 recommendation...\n",
       "2  33979791  ...  segment lesion region compute tomography ct im...\n",
       "3  33979779  ...  background to suppress transmission government...\n",
       "4  33979771  ...  effect relapse multiple sclerosis m unknown re...\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhGDEGQcGdXJ",
    "outputId": "6e4393f2-4648-495a-f0b9-7e496cd9c44a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERAL PURPOSE: To introduce the 15 recommendations of the International Ostomy Guideline (IOG) 2020, covering the four key arenas of education, holistic aspects, and pre- and postoperative care; and to summarize key concepts for clinicians to customize for translation into their practice. TARGET AUDIENCE: This continuing education activity is intended for physicians, physician assistants, nurse practitioners, and nurses with an interest in skin and wound care. LEARNING OBJECTIVES/OUTCOMES: After participating in this educational activity, the participant will:1. Analyze supporting evidence for the education recommendations in the IOG 2020.2. Identify a benefit of the International Charter of Ostomate Rights.3. Distinguish concepts related to pre- and postoperative ostomy-related care.4. Select a potential barrier to IOG 2020 guideline implementation.\n",
      "\n",
      "general purpose to introduce 15 recommendation international ostomy guideline iog 2020 cover key arena education holistic aspect pre- postoperative care summarize key concept clinician customize translation practice target audience this continue education activity intend physician physician assistant nurse practitioner nurse interest skin wound care learning objective / outcomes after participate educational activity participant will:1 analyze support evidence education recommendation iog 2020.2 identify benefit international charter ostomate rights.3 distinguish concept relate pre- postoperative ostomy relate care.4 select potential barrier iog 2020 guideline implementation\n",
      "\n",
      "general purpose to introduce 15 recommendation international ostomy guideline iog cover key arena education holistic aspect pre- postoperative summarize key concept clinician customize translation practice target audience continue education activity intend physician physician assistant nurse practitioner nurse interest skin wound learning objective / outcome after participate educational activity participant will:1 analyze support evidence education recommendation iog 2020.2 identify benefit international charter ostomate rights.3 distinguish concept relate pre- postoperative ostomy relate care.4 select potential barrier iog guideline implementation\n",
      "----------------------------------------------------------\n",
      "Segmenting lesion regions of Coronavirus Disease 2019 (COVID-19) from computed tomography (CT) images is a challenge owing to COVID-19 lesions characterized by high variation, low contrast between infection lesions and around normal tissues, and blurred boundaries of infections. Moreover, a shortage of available CT dataset hinders deep learning techniques applying to tackling COVID-19. To address these issues, we propose a deep learning-based approach known as PPM-Unet to segmenting COVID-19 lesions from CT images. Our method improves an Unet by adopting pyramid pooling modules instead of the conventional skip connection and then enhances the representation of the neural network by aiding the global attention mechanism. We first pre-train PPM-Unet on COVID-19 dataset of pseudo labels containing1600 samples producing a coarse model. Then we fine-tune the coarse PPM-Unet on the standard COVID-19 dataset consisting of 100 pairs of samples to achieve a fine PPM-Unet. Qualitative and quantitative results demonstrate that our method can accurately segment COVID-19 infection regions from CT images, and achieve higher performance than other state-of-the-art segmentation models in this study. It offers a promising tool to lay a foundation for quantitatively detecting COVID-19 lesions.\n",
      "\n",
      "segment lesion region coronavirus disease 2019 covid-19 compute tomography ct image challenge owe covid-19 lesion characterize high variation low contrast infection lesion normal tissue blur boundary infection moreover shortage available ct dataset hinder deep learning technique apply tackle covid-19 to address issue propose deep learning base approach know ppm unet segment covid-19 lesion ct image our method improve unet adopt pyramid pooling module instead conventional skip connection enhance representation neural network aid global attention mechanism we pre - train ppm unet covid-19 dataset pseudo label containing1600 sample produce coarse model then fine tune coarse ppm unet standard covid-19 dataset consist 100 pair sample achieve fine ppm unet qualitative quantitative result demonstrate method accurately segment covid-19 infection region ct image achieve high performance state art segmentation model study it offer promising tool lay foundation quantitatively detect covid-19 lesion\n",
      "\n",
      "segment lesion region compute tomography ct image challenge owe lesion characterize variation low contrast lesion normal tissue blur boundary moreover shortage available ct dataset hinder deep learning technique apply tackle to address issue propose deep learning base approach know ppm unet segment lesion ct image our improve unet adopt pyramid pooling module instead conventional skip connection enhance representation neural network aid global attention mechanism pre train ppm unet dataset pseudo label containing1600 sample produce coarse model then fine tune coarse ppm unet standard dataset consist 100 pair sample achieve fine ppm unet qualitative quantitative demonstrate accurately segment region ct image achieve performance state art segmentation model it offer promising tool lay foundation quantitatively detect lesion\n",
      "----------------------------------------------------------\n",
      "BACKGROUND: To suppress the transmission of coronavirus, many governments, including that of the island of Ireland, implemented a societal lockdown, which included school closures, limits on social gatherings, and time outdoors. This study aimed to evaluate changes in physical activity (PA), mental health, sleep, and social media use among adolescent girls during lockdown. METHODS: 281 female pupils (12-14 y) taking part in the ongoing Walking In Schools study on the island of Ireland self-reported PA, mental health, sleep, and social media use before (September-October 2019) and during lockdown (May-June 2020), via questionnaires. These were supplemented with open-ended structured interviews conducted with 16 girls during lockdown. RESULTS: During the period of lockdown and school closures, pupils tried new forms of PA and undertook PA with family, but there was no significant change in self-reported PA. There was a decline in health-related quality of life and motivation for exercise; however, self-efficacy for walking and happiness with appearance increased. There was no change in sleep quality or social media usage. CONCLUSIONS: Despite the many challenges that schools face as they reopen, there is a need to continue to prioritize PA and motivation for exercise to support health and well-being in adolescent girls.\n",
      "\n",
      "background to suppress transmission coronavirus government include island ireland implement societal lockdown include school closure limit social gathering time outdoors this study aim evaluate change physical activity pa mental health sleep social medium use adolescent girl lockdown method 281 female pupil 12 - 14 y take ongoing walking in schools study island ireland self report pa mental health sleep social medium use september october 2019 lockdown may june 2020 questionnaire these supplement open end structured interview conduct 16 girl lockdown result during period lockdown school closure pupil try new form pa undertake pa family significant change self report pa there decline health relate quality life motivation exercise self efficacy walking happiness appearance increase there change sleep quality social medium usage conclusion despite challenge school face reopen need continue prioritize pa motivation exercise support health adolescent girl\n",
      "\n",
      "background to suppress transmission government island ireland implement societal lockdown school closure limit social gathering time outdoors aim evaluate change physical activity pa mental sleep social medium use adolescent girl lockdown 281 female pupil 12 14 y take ongoing walking school island ireland self pa mental sleep social medium use september october lockdown may june questionnaire these supplement open end structured interview conduct 16 girl lockdown during period lockdown school closure pupil try new form pa undertake pa family significant change self pa there decline relate quality life motivation exercise self efficacy walking happiness appearance increase there change sleep quality social medium usage conclusion despite challenge school face reopen need continue prioritize pa motivation exercise support adolescent girl\n",
      "----------------------------------------------------------\n",
      "The effect of coronavirus disease (COVID-19) on the risk of relapse in multiple sclerosis (MS) have been unknown. In this retrospective study of 41 relapsing-remitting MS patients, number of relapses in pre-defined at risk-period (ARP) was compared with the previous two years. During the previous two years, a total of 32 attacks was reported, which 5 (15.6%) were during the at-risk period. After adjusting for age and sex, there was an increased risk of attack during ARP compared to the previous two years (RR: 2.566, 95%CI: 1.075-6.124, P=0.034). Our preliminary study suggested that COVID-19 can trigger exacerbation of MS.\n",
      "\n",
      "the effect coronavirus disease covid-19 risk relapse multiple sclerosis ms unknown in retrospective study 41 relapse remit ms patient number relapse pre - define risk period arp compare previous year during previous year total 32 attack report 5 15.6 % risk period after adjust age sex increased risk attack arp compare previous year rr 2.566 95%ci 1.075 6.124 p=0.034 our preliminary study suggest covid-19 trigger exacerbation ms\n",
      "\n",
      "effect relapse multiple sclerosis m unknown retrospective 41 relapse remit m number relapse pre define period arp compare previous year during previous year total 32 attack 5 15.6 period after adjust age sex increased attack arp compare previous year rr 2.566 95%ci 1.075 6.124 p=0.034 our preliminary suggest trigger exacerbation m\n",
      "----------------------------------------------------------\n",
      "Coronavirus disease 2019 (COVID-19) patients have manifested a variety of neurological complications, and there is still much to reveal regarding the neurotropism of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Human stem cell-derived brain organoids offer a valuable in vitro approach to study the cellular effects of SARS-CoV-2 on the brain. Here we used human embryonic stem cell-derived cortical organoids to investigate whether SARS-CoV-2 could infect brain tissue in vitro and found that cortical organoids could be infected at low viral titers and within 6 h. Importantly, we show that glial cells and cells of the choroid plexus were preferentially targeted in our model, but not neurons. Interestingly, we also found expression of angiotensin-converting enzyme 2 in SARS-CoV-2 infected cells; however, viral replication and cell death involving DNA fragmentation does not occur. We believe that our model is a tractable platform to study the cellular effects of SARS-CoV-2 infection in brain tissue.\n",
      "\n",
      "coronavirus disease 2019 covid-19 patient manifest variety neurological complication reveal neurotropism severe acute respiratory syndrome coronavirus 2 sars cov-2 human stem cell derive brain organoid offer valuable vitro approach study cellular effect sars cov-2 brain here human embryonic stem cell derive cortical organoid investigate sars cov-2 infect brain tissue vitro find cortical organoid infect low viral titer 6 h. importantly glial cell cell choroid plexus preferentially target model neuron interestingly find expression angiotensin convert enzyme 2 sars cov-2 infect cell viral replication cell death involve dna fragmentation occur we believe model tractable platform study cellular effect sars cov-2 infection brain tissue\n",
      "\n",
      "manifest variety neurological complication reveal neurotropism acute syndrome 2 human stem cell derive brain organoid offer valuable vitro approach cellular effect brain here human embryonic stem cell derive cortical organoid investigate infect brain tissue vitro find cortical organoid infect low viral titer 6 h. importantly glial cell cell choroid plexus preferentially target model neuron interestingly find expression angiotensin convert enzyme 2 infect cell viral replication cell death involve dna fragmentation occur believe model tractable platform cellular effect brain tissue\n",
      "----------------------------------------------------------\n",
      "We detected severe acute respiratory syndrome coronavirus 2 in an otherwise healthy poodle living with 4 family members who had coronavirus disease. We observed antibodies in serum samples taken from the dog, indicating seroconversion. Full-length genome sequencing showed that the canine and human viruses were identical, suggesting human-to-animal transmission.\n",
      "\n",
      "we detect severe acute respiratory syndrome coronavirus 2 healthy poodle live 4 family member coronavirus disease we observe antibody serum sample take dog indicate seroconversion full length genome sequencing show canine human virus identical suggest human animal transmission\n",
      "\n",
      "detect acute syndrome 2 healthy poodle live 4 family member observe antibody serum sample take dog indicate seroconversion full length genome sequencing show canine human identical suggest human animal transmission\n",
      "----------------------------------------------------------\n",
      "We describe response measures to an outbreak involving 128 (33.4%) coronavirus disease cases (46.1% asymptomatic) among 383 persons onboard a passenger ship. Multivariate analysis indicated that dining in certain rooms and bar areas, nationality, working department (for crew members), and quarantining onboard the ship were significantly associated with infection.\n",
      "\n",
      "we describe response measure outbreak involve 128 33.4 % coronavirus disease case 46.1 % asymptomatic 383 person onboard passenger ship multivariate analysis indicate dining certain room bar area nationality working department crew member quarantine onboard ship significantly associate infection\n",
      "\n",
      "describe response measure outbreak involve 128 33.4 46.1 asymptomatic 383 person onboard passenger ship multivariate analysis indicate dining certain room bar area nationality working department crew member quarantine onboard ship significantly associate\n",
      "----------------------------------------------------------\n",
      "Worksites with on-site operations have experienced coronavirus disease (COVID-19) outbreaks. We analyzed data for 698 nonresidential, nonhealthcare worksite COVID-19 outbreaks investigated in Los Angeles County, California, USA, during March 19, 2020September 30, 2020, by using North American Industry Classification System sectors and subsectors. Nearly 60% of these outbreaks occurred in 3 sectors: manufacturing (n = 184, 26.4%), retail trade (n = 137, 19.6%), and transportation and warehousing (n = 73, 10.5%). The largest number of outbreaks and largest number and highest incidence rate of outbreak-associated cases occurred in manufacturing. Furthermore, 7 of the 10 industry subsectors with the highest incidence rates were within manufacturing. Approximately 70% of outbreak-associated case-patients reported Hispanic ethnicity. Facilities employing more on-site staff had larger and longer outbreaks. Identification of highly affected industry sectors and subsectors is necessary for targeted public health planning, outreach, and response, including ensuring vaccine access, to reduce burden of COVID-19 in vulnerable workers.\n",
      "\n",
      "worksite site operation experience coronavirus disease covid-19 outbreak we analyze datum 698 nonresidential nonhealthcare worksite covid-19 outbreak investigate los angeles county california usa march 19 2020september 30 2020 north american industry classification system sector subsector nearly 60 % outbreak occur 3 sector manufacturing n = 184 26.4 % retail trade n = 137 19.6 % transportation warehousing n = 73 10.5 % the large number outbreak large number high incidence rate outbreak associate case occur manufacturing furthermore 7 10 industry subsector high incidence rate manufacturing approximately 70 % outbreak associate case patient report hispanic ethnicity facility employ site staff large long outbreak identification highly affected industry sector subsector necessary target public health planning outreach response include ensure vaccine access reduce burden covid-19 vulnerable worker\n",
      "\n",
      "worksite site operation experience outbreak analyze datum 698 nonresidential nonhealthcare worksite outbreak investigate los angeles county california usa march 19 2020september 30 north american industry classification system sector subsector nearly 60 outbreak occur 3 sector manufacturing n = 184 26.4 retail trade n = 137 19.6 transportation warehousing n = 73 10.5 large number outbreak large number incidence rate outbreak associate occur manufacturing furthermore 7 10 industry subsector incidence rate manufacturing approximately 70 outbreak associate hispanic ethnicity facility employ site staff large long outbreak identification highly affected industry sector subsector necessary target public planning outreach response ensure vaccine access reduce burden vulnerable worker\n",
      "----------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "processed_text = df[\"processed_text\"]\n",
    "body_text = df[\"body_text\"]\n",
    "text_lemmatized = df[\"text_lemmatized\"]\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(body_text[i])\n",
    "    print(\"\")\n",
    "    print(processed_text[i])\n",
    "    print(\"\")\n",
    "    print(text_lemmatized[i])\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt1Md_7TGdXJ"
   },
   "source": [
    "### Removal of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qxnNA9MqGdXJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "302r7FtTGdXJ",
    "outputId": "3a6eb8b8-cfb7-47ce-b5f4-d37e23eed0fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract_summary</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "      <th>language</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_lemmatized_no_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33979833</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Finkel Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Severe acute respiratory syndrome&lt;br&gt;coronavi...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>125</td>\n",
       "      <td>en</td>\n",
       "      <td>severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33979817</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>Chabal LO</td>\n",
       "      <td>P</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15&lt;br&gt;recom...</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>85</td>\n",
       "      <td>en</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33979791</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Ma Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus&lt;br&gt;D...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>133</td>\n",
       "      <td>en</td>\n",
       "      <td>segment lesion region coronavirus disease 2019...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33979779</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>O'Kane SM</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of&lt;b...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>130</td>\n",
       "      <td>en</td>\n",
       "      <td>background to suppress transmission coronaviru...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33979771</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>Barzegar M</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19)&lt;...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>78</td>\n",
       "      <td>en</td>\n",
       "      <td>the effect coronavirus disease covid-19 risk r...</td>\n",
       "      <td>effect relapse multiple sclerosis ms unknown r...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  ...                             text_lemmatized_no_url\n",
       "0  33979833  ...  acute syndrome 2 cause ongoing 19 pandemic(1 c...\n",
       "1  33979817  ...  general purpose to introduce 15 recommendation...\n",
       "2  33979791  ...  segment lesion region compute tomography ct im...\n",
       "3  33979779  ...  background to suppress transmission government...\n",
       "4  33979771  ...  effect relapse multiple sclerosis m unknown re...\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_lemmatized_no_url\"] = df[\"text_lemmatized\"].apply(lambda text: remove_urls(text))\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2ebpR2QGdXK"
   },
   "source": [
    "### Removal of HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "pFH-0vTVGdXK"
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sytMw93LGdXK",
    "outputId": "8220c941-b128-432b-b8aa-d817c4786c00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract_summary</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "      <th>language</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_lemmatized_no_url</th>\n",
       "      <th>text_lemmatized_url_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33979833</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>Finkel Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Severe acute respiratory syndrome&lt;br&gt;coronavi...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>125</td>\n",
       "      <td>en</td>\n",
       "      <td>severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "      <td>acute syndrome 2 cause ongoing 19 pandemic(1 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33979817</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15 recommend...</td>\n",
       "      <td>Chabal LO</td>\n",
       "      <td>P</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>GENERAL PURPOSE: To introduce the 15&lt;br&gt;recom...</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>85</td>\n",
       "      <td>en</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "      <td>general purpose to introduce 15 recommendation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33979791</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus Disea...</td>\n",
       "      <td>Ma Y</td>\n",
       "      <td>S</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Segmenting lesion regions of Coronavirus&lt;br&gt;D...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>133</td>\n",
       "      <td>en</td>\n",
       "      <td>segment lesion region coronavirus disease 2019...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "      <td>segment lesion region compute tomography ct im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33979779</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of co...</td>\n",
       "      <td>O'Kane SM</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>BACKGROUND: To suppress the transmission of&lt;b...</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>130</td>\n",
       "      <td>en</td>\n",
       "      <td>background to suppress transmission coronaviru...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "      <td>background to suppress transmission government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33979771</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19) o...</td>\n",
       "      <td>Barzegar M</td>\n",
       "      <td>C</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>The effect of coronavirus disease (COVID-19)&lt;...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>78</td>\n",
       "      <td>en</td>\n",
       "      <td>the effect coronavirus disease covid-19 risk r...</td>\n",
       "      <td>effect relapse multiple sclerosis ms unknown r...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "      <td>effect relapse multiple sclerosis m unknown re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  ...                           text_lemmatized_url_html\n",
       "0  33979833  ...  acute syndrome 2 cause ongoing 19 pandemic(1 c...\n",
       "1  33979817  ...  general purpose to introduce 15 recommendation...\n",
       "2  33979791  ...  segment lesion region compute tomography ct im...\n",
       "3  33979779  ...  background to suppress transmission government...\n",
       "4  33979771  ...  effect relapse multiple sclerosis m unknown re...\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_lemmatized_url_html\"] = df[\"text_lemmatized_no_url\"].apply(lambda text: remove_html(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADe6yiQUGdXK"
   },
   "source": [
    "### Let's take a look at word count in the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "QkCHs7YzGdXL"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CEkqhYSoGdXL",
    "outputId": "bb37f339-d12d-4467-fc26-990586fae440"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    71179.000000\n",
       "mean       207.227455\n",
       "std         89.059328\n",
       "min          0.000000\n",
       "25%        149.000000\n",
       "50%        205.000000\n",
       "75%        254.000000\n",
       "max       1899.000000\n",
       "Name: body_word_count, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['body_word_count'])\n",
    "df['body_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UJl1yVcGGdXL",
    "outputId": "c43cee48-1af9-4522-9f5f-2e3d6e6c309f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    71179.000000\n",
       "mean       134.100788\n",
       "std         47.220880\n",
       "min          0.000000\n",
       "25%        104.000000\n",
       "50%        135.000000\n",
       "75%        162.000000\n",
       "max        894.000000\n",
       "Name: body_unique_words, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['body_unique_words'])\n",
    "df['body_unique_words'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaXzOqs3GdXL"
   },
   "source": [
    "These two plots give us a good idea of the content we are dealing with. Most papers are about 5000 words in length. The long tails in both plots are caused by outliers. In fact, ~98% of the papers are under 20,000 words in length while a select few are over 200,000! <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUv4qKBuGdXM"
   },
   "source": [
    "# Vectorization\n",
    "\n",
    "Now that we have pre-processed the data, it is time to convert it into a format that can be handled by our algorithms. For this purpose we will be using tf-idf. This will convert our string formatted data into a measure of how important each word is to the instance out of the literature as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pwPl6xfGdXM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def vectorize(text, maxx_features):\n",
    "    \n",
    "    global vectorizer \n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3bxyiTZGdXM"
   },
   "source": [
    "Vectorize our data. We will be clustering based off the content of the body text. The maximum number of features will be limited. Only the top 2 ** 12 features will be used, eseentially acting as a noise filter. Additionally, more features cause painfully long runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtfWSBp7GdXM"
   },
   "outputs": [],
   "source": [
    "text = df['text_lemmatized_url_html'].values\n",
    "vectorizer = TfidfVectorizer(max_features= 2 ** 12)\n",
    "X = vectorize(text, 2 ** 12)\n",
    "X.shape\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3WWtZKZGdXV"
   },
   "source": [
    "# PCA  & Clustering\n",
    "\n",
    "Let's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. The reason for this is that by keeping a large number of dimensions with PCA, you don’t destroy much of the information, but hopefully will remove some noise/outliers from the data, and make the clustering problem easier for k-means. Note that X_reduced will only be used for k-means, t-SNE will still use the original feature vector X that was generated through tf-idf on the NLP processed text.\n",
    "\n",
    "(Thank you Dr. Edward Raff for the suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjItIrzqGdXV"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced= pca.fit_transform(X.toarray())\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1zncLGKGdXW"
   },
   "source": [
    "To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqhRR064GdXW"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfzxgKSbGdXW"
   },
   "source": [
    "### How many clusters? \n",
    "\n",
    "To find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQtFIem3GdXW"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# run kmeans with many different k\n",
    "distortions = []\n",
    "K = range(2, 10)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n",
    "    k_means.fit(X_reduced)\n",
    "    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('Found distortion for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlIZDecZGdXX"
   },
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEReELpFGdXX"
   },
   "source": [
    "In this plot we can see that the better k values are between 18-25. After that, the decrease in distortion is not as significant. For simplicity, we will use k=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmwV8zp9GdXX"
   },
   "source": [
    "### Run k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAJHJvcJGdXX"
   },
   "source": [
    "Now that we have an appropriate k value, we can run k-means on the PCA-processed feature vector (X_reduced). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z57mgFgyGdXY"
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_reduced)\n",
    "df['y'] = y_pred\n",
    "df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELUJxg91GdXY"
   },
   "source": [
    "# Dimensionality Reduction with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7viOoUkGdXY"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSPlTLQ_GdXY"
   },
   "source": [
    "So that step took a while! Let's take a look at what our data looks like when compressed to 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRXeDaCTGdXZ"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "plt.title('t-SNE with no Labels')\n",
    "plt.savefig(\"t-sne_covid19.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuG7CYq1GdXZ"
   },
   "source": [
    "This looks pretty bland. There are some clusters we can immediately detect, but the many instances closer to the center are harder to separate. t-SNE did a good job at reducing the dimensionality, but now we need some labels. Let's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Va-oH-KGdXZ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(5, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0_CsDsTGdXZ"
   },
   "source": [
    "The labeled plot gives better insight into how the papers are grouped. It is interesting that both k-means and t-SNE are able to agree on certain clusters even though they were ran independetly. The location of each paper on the plot was determined by t-SNE while the label (color) was determined by k-means. If we look at a particular part of the plot where t-SNE has grouped many articles forming a cluster, it is likely that k-means is uniform in the labeling of this cluster (most of the cluster is the same color). This behavior shows that structure within the literature can be observed and measured to some extent. \n",
    "\n",
    "Now there are other cases where the colored labels (k-means) are spread out on the plot (t-SNE). This is a result of t-SNE and k-means finding different connections in the higher dimensional data. The topics of these papers often intersect so it hard to cleanly separate them. This effect can be observed in the formation of subclusters on the plot. These subclusters are a conglomeration of different k-means labels but may share some connection determined by t-SNE.\n",
    "\n",
    "This organization of the data does not act as a simple search engine. The clustering + dimensionality reduction is performed on the mathematical similarities of the publications. As an unsupervised approach, the algorithms may even find connections that were unnaparent to humans. This may highlight hidden shared information and advance further research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlQ1QWQ3GdXa"
   },
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-b9yEPeGdXa"
   },
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz\n",
    "import en_ner_bc5cdr_md\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uT5XlpxGdXa"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "nmf = NMF(n_components=20, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv3zcGECGdXa"
   },
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(5, 4, figsize=(30, 60), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTdCJFkpGdXb"
   },
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(max_features= 2 ** 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYgUClytGdXb"
   },
   "outputs": [],
   "source": [
    "tfidf_feature_names = vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, 40,\n",
    "               'Topics in NMF model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmd86A4KGdXb"
   },
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    #top words = 100\n",
    "    top_features_ind = topic.argsort()[:-40 - 1:-1]\n",
    "    top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "    all_keywords.append(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjR4CdXRGdXb"
   },
   "outputs": [],
   "source": [
    "for num_clus in range(0,20):\n",
    "    print(\"*************Cluster \",num_clus,\"*********************\")\n",
    "    str1 = ','.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    print(str1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    TERM_LIST = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    print(\"TERM_LIST : \" ,TERM_LIST )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZgQfDUdGdXc"
   },
   "source": [
    "# Word's Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwT8KzhtGdXc"
   },
   "outputs": [],
   "source": [
    "! python3 -m spacy download en_core_web_md\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "cov_word =nlp(\"coronavirus\")\n",
    "for num_clus in range(0,20):\n",
    "    print(\"*************Cluster \",num_clus,\"*********************\")\n",
    "    str1 = ' '.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    print(str1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    words = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    print(\"TERM_LIST : \" ,words)\n",
    "    print(\"\")\n",
    "    for token in doc:\n",
    "        #print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
    "        if cov_word.similarity(token) > 0.5 :\n",
    "            print(\"Similarity Between {\",cov_word,\"} and {\",token,\"} is :   \",cov_word.similarity(token))\n",
    "    print(\"__________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5My4g8nmGdXc"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "cov_word =nlp(\"coronavirus\")\n",
    "top_similar = []\n",
    "for num_clus in range(0,20):\n",
    "    str1 = ' '.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    words = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    for token in doc:\n",
    "        if cov_word.similarity(token) > 0.5 :\n",
    "            top_similar.append((token.text , cov_word.similarity(token)))\n",
    "LL = top_similar\n",
    "top_similar_sorted = []\n",
    "for a in top_similar :\n",
    "    if a not in top_similar_sorted:\n",
    "        top_similar_sorted.append(a)\n",
    "top_similar_sorted = sorted(top_similar_sorted, key=lambda tup: tup[1],reverse=True)\n",
    "top_similar_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJygphmLGdXc"
   },
   "outputs": [],
   "source": [
    "li = top_similar_sorted[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mPLisYIGdXd"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "lii = []\n",
    "for tup in li:\n",
    "    cov_word =nlp(tup[0])\n",
    "    top_similar = []\n",
    "    for num_clus in range(0,20):\n",
    "        str1 = ' '.join(all_keywords[num_clus])\n",
    "        doc = nlp(str1)\n",
    "        words = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "        for token in doc:\n",
    "            if cov_word.similarity(token) > 0.5 :\n",
    "                top_similar.append((token.text , cov_word.similarity(token)))\n",
    "    top_similar_sorted = []\n",
    "    for a in top_similar :\n",
    "        if a not in top_similar_sorted:\n",
    "            top_similar_sorted.append(a)\n",
    "    top_similar_sorted = sorted(top_similar_sorted, key=lambda tup: tup[1],reverse=True)\n",
    "    top_similar_sorted = top_similar_sorted[0:10]\n",
    "    lii.append((tup[0],top_similar_sorted))\n",
    "# LL = top_similar\n",
    "# top_similar_sorted = []\n",
    "# for a in top_similar :\n",
    "#     if a not in top_similar_sorted:\n",
    "#         top_similar_sorted.append(a)\n",
    "# top_similar_sorted = sorted(top_similar_sorted, key=lambda tup: tup[1],reverse=True)\n",
    "# top_similar_sorted\n",
    "top_ten_of_ten = lii\n",
    "top_ten_of_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwuyS2zdGdXd"
   },
   "outputs": [],
   "source": [
    "\n",
    "ten_diseases = []\n",
    "for i,mots in top_similar_sorted :\n",
    "    if mots not in ten_diseases:\n",
    "        ten_diseases.append(i)\n",
    "ten_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlwGYOpTGdXe"
   },
   "outputs": [],
   "source": [
    "hundred_diseases = []\n",
    "for i,mots in top_ten_of_ten :\n",
    "    for mot,val in mots:\n",
    "        if mot not in hundred_diseases:\n",
    "            hundred_diseases.append(mot)\n",
    "hundred_diseases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whrbctpAGdXe"
   },
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"diabete\"))\n",
    "print(lemmatizer.lemmatize(\"diabetes\"))\n",
    "print(lemmatizer.lemmatize(\"diabetic\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"vaccinations\"))\n",
    "print(lemmatizer.lemmatize(\"vaccination\"))\n",
    "print(lemmatizer.lemmatize(\"vaccinated\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"speaking\"))\n",
    "print(lemmatizer.lemmatize(\"speaks\"))\n",
    "print(lemmatizer.lemmatize(\"spoken\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFtc50BvGdXf"
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKg6ZvBJGdXg"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vcy4mJ2NGdXh"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')\n",
    "tff = tf.fit_transform(text)\n",
    "lda = LatentDirichletAllocation(n_components=20, max_iter=10, learning_method='online', random_state=42)\n",
    "lda.fit(tff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi1R3ksKGdXi"
   },
   "outputs": [],
   "source": [
    "tf_feature_names = tf.get_feature_names()\n",
    "plot_top_words(lda, tf_feature_names, 40, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rLQ6z1OGdXj"
   },
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    #top words = 100\n",
    "    top_features_ind = topic.argsort()[:-100 - 1:-1]\n",
    "    top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "    all_keywords.append(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC911PdEGdXj"
   },
   "outputs": [],
   "source": [
    "for num_clus in range(0,20):\n",
    "    print(\"*************Cluster \",num_clus,\"*********************\")\n",
    "    str1 = ','.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    print(str1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    TERM_LIST = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    print(\"TERM_LIST : \" ,TERM_LIST)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ23mdGJGdXj"
   },
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4SplcohGdXj"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMJi5v-eGdXk"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twjr_ruVGdXk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0] ,\" \",end='')\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opHly9WoGdXk"
   },
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for topic_idx, topic in enumerate(svd_model.components_):\n",
    "    #top words = 100\n",
    "    top_features_ind = topic.argsort()[:-40 - 1:-1]\n",
    "    top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "    all_keywords.append(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xh9_N_sQGdXk"
   },
   "outputs": [],
   "source": [
    "for num_clus in range(0,19):\n",
    "    print(\"*************Cluster \",num_clus,\"*********************\")\n",
    "    str1 = ','.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    print(str1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    TERM_LIST = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    print(\"TERM_LIST : \" ,TERM_LIST )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xLP_e6OGdXl"
   },
   "source": [
    "# Topic Modeling on Each Cluster\n",
    "\n",
    "Now we will attempt to find the most significant words in each clusters. K-means clustered the articles but did not label the topics. Through topic modeling we will find out what the most important terms for each cluster are. This will add more meaning to the cluster by giving keywords to quickly identify the themes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIYgILD4GdXl"
   },
   "source": [
    "For topic modeling, we will use LDA (Latent Dirichlet Allocation). In LDA, each document can be described by a distribution of topics and each topic can be described by a distribution of words[.](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhRnfxi3GdXl"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ImkN2FzGdXl"
   },
   "source": [
    "First we will create 20 vectorizers, one for each of our cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4reePRHjGdXm"
   },
   "outputs": [],
   "source": [
    "vectorizers = []\n",
    "    \n",
    "for ii in range(0, 5):\n",
    "    # Creating a vectorizer\n",
    "    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0ehZYYDGdXm"
   },
   "source": [
    "Now we will vectorize the data from each of our clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqeWNw16GdXm"
   },
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "\n",
    "for current_cluster, cvec in enumerate(vectorizers):\n",
    "    try:\n",
    "        vectorized_data.append(cvec.fit_transform(df.loc[df['y'] == current_cluster, 'text_lemmatized_url_html']))\n",
    "        #d'ou vient df['y'] =   y_pred = kmeans.fit_predict(X_reduced)      df['y'] = y_pred       df['y']\n",
    "    except Exception as e:\n",
    "        print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
    "        vectorized_data.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrLjsueEGdXm"
   },
   "outputs": [],
   "source": [
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCbIuEnTGdXn"
   },
   "source": [
    "Topic modeling will be performed through the use of Latent Dirichlet Allocation (LDA). This is a generative statistical model that allows sets of words to be explained by a shared topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjjMs_KiGdXn"
   },
   "source": [
    "# Topic modeling on each cluster using LDA : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjxM-t3oGdXn"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iadtccWGdXn"
   },
   "outputs": [],
   "source": [
    "# number of topics per cluster\n",
    "NUM_TOPICS_PER_CLUSTER = 6\n",
    "\n",
    "lda_models = []\n",
    "for ii in range(0, 5):\n",
    "    # Latent Dirichlet Allocation Model\n",
    "    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n",
    "    lda_models.append(lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKOAm3CQGdXo"
   },
   "source": [
    "For each cluster, we had created a correspoding LDA model in the previous step. We will now fit_transform all the LDA models on their respective cluster vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YMlSzi5GdXo"
   },
   "outputs": [],
   "source": [
    "clusters_lda_data = []\n",
    "\n",
    "for current_cluster, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_cluster))\n",
    "    \n",
    "    if vectorized_data[current_cluster] != None:\n",
    "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hElpEbxcGdXo"
   },
   "source": [
    "Extracts the keywords from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQtSif0LGdXo"
   },
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=20):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "                \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO42LqrQGdXo"
   },
   "source": [
    "Append list of keywords for a single cluster to 2D list of length NUM_TOPICS_PER_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1kCDB0iGdXp"
   },
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "\n",
    "    if vectorized_data[current_vectorizer] != None:\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vsPJpVNGdXp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(all_keywords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxvVBe0uGdXp"
   },
   "outputs": [],
   "source": [
    "for num_clus in range(0,5):\n",
    "    print(\"*************Cluster \",num_clus,\"*********************\")\n",
    "    str1 = ','.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    print(str1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    TERM_LIST = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    print(\"TERM_LIST : \" ,TERM_LIST )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "expLMIYXGdXq"
   },
   "source": [
    "# Topic modeling on each cluster using NMF : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDvtJo89GdXq"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqOid8w1GdXq"
   },
   "outputs": [],
   "source": [
    "# number of topics per cluster\n",
    "NUM_TOPICS_PER_CLUSTER = 6\n",
    "\n",
    "nmf_models = []\n",
    "for ii in range(0, 5):\n",
    "    # nmf\n",
    "    nmf = NMF(n_components=NUM_TOPICS_PER_CLUSTER, random_state=1,alpha=.1, l1_ratio=.5)\n",
    "    nmf_models.append(nmf)\n",
    "    \n",
    "nmf_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kSWIhkVGdXq"
   },
   "outputs": [],
   "source": [
    "clusters_nmf_data = []\n",
    "\n",
    "for current_cluster, nmf in enumerate(nmf_models):\n",
    "    print(\"Current Cluster: \" + str(current_cluster))\n",
    "    if vectorized_data[current_cluster] != None:\n",
    "        clusters_nmf_data.append((nmf.fit_transform(vectorized_data[current_cluster])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eoa7zpV9GdXr"
   },
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=20):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "                \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxWyYVYIGdXr"
   },
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for current_vectorizer, nmf in enumerate(nmf_models):\n",
    "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "    if vectorized_data[current_vectorizer] != None:\n",
    "        all_keywords.append(selected_topics(nmf, vectorizers[current_vectorizer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnQnh6yJGdXr"
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(all_keywords[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbCDnySBGdXs"
   },
   "source": [
    "## travail realisé :\n",
    " - detection automatique des noms de maladies\n",
    " - construction d'un corpus en utilisant les noms des maladies detectés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anlH_PPwGdXs"
   },
   "outputs": [],
   "source": [
    "for num_clus in range(0,5):\n",
    "    print(\"*************Cluster \",num_clus,\"*********************\")\n",
    "    str1 = ','.join(all_keywords[num_clus])\n",
    "    doc = nlp(str1)\n",
    "    print(str1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    TERM_LIST = [e.text for e in doc.ents if (e.label_ == \"DISEASE\")]\n",
    "    print(\"TERM_LIST : \" ,TERM_LIST )\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1wFmFYcGdXs"
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import json\n",
    "\n",
    "\n",
    "MAX_COUNT = 3\n",
    "for TERM in ten_diseases:\n",
    "    Entrez.email = 'A.N.Other@example.com'\n",
    "    h = Entrez.esearch(db='pubmed', retmax=MAX_COUNT, term=TERM)\n",
    "    result = Entrez.read(h)\n",
    "    ids = result['IdList']\n",
    "    h = Entrez.efetch(db='pubmed', id=ids, rettype='medline', retmode='json')\n",
    "    records = Medline.parse(h)\n",
    "    i=0\n",
    "    for record in records:\n",
    "        filename=\"fichier_\"+TERM+str(i)+\".json\"\n",
    "        json.dump(record, open(\".\\\\new_corpus\\\\\"+filename, 'w'))\n",
    "        data = json.load(open(\".\\\\new_corpus\\\\\"+filename))\n",
    "        i=i+1\n",
    "[(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3HSAiA5GdXs"
   },
   "outputs": [],
   "source": [
    "len(all_keywords)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "covid-19-literature-clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
